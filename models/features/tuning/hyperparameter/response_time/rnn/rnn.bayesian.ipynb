{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning RNN Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Add path to the root folder\n",
    "sys.path.append('../../../../../../')\n",
    "sys.path.append('../../../../../features/prediction/')\n",
    "\n",
    "\n",
    "from models.features.prediction.putils.formatter import create_sequences\n",
    "from models.features.prediction.config.control import CONFIG\n",
    "from models.features.prediction.config.path import BASE_DATASET_PATH\n",
    "from models.features.prediction.manager import DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNHyperparameterTuner:\n",
    "    def __init__(self, dataset, size_train, size_test):\n",
    "        self.size_train = size_train\n",
    "        self.size_test = size_test\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.dataset = dataset\n",
    "        self.training_dataset = dataset[:size_train]\n",
    "        self.scaled_training_dataset = self.scaler.fit_transform(\n",
    "            self.training_dataset.values.reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "    def train_model(self, config):\n",
    "        # Group data for RNN\n",
    "        X, y = create_sequences(\n",
    "            self.scaled_training_dataset,\n",
    "            config.get(\"n_past\", 5),\n",
    "            config.get(\"steps\", 1),\n",
    "        )\n",
    "\n",
    "        # RNN Model\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            SimpleRNN(\n",
    "                config.get(\"neurons_l1\", 50),\n",
    "                activation=config.get(\"activation_function_l1\", \"relu\"),\n",
    "                input_shape=(X.shape[1], X.shape[2]),\n",
    "                return_sequences=True,\n",
    "            )\n",
    "        )\n",
    "        model.add(SimpleRNN(config.get(\"neurons_l2\", 50), activation=config.get(\"activation_function_l2\", \"relu\")))\n",
    "        model.add(Dense(y.shape[1]))\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=config.get(\"learning_rate\", 0.001)\n",
    "            ),\n",
    "            loss=\"mse\",\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            epochs=config.get(\"epochs\", 1),\n",
    "            verbose=config.get(\"verbose\", 0),\n",
    "            batch_size=config.get(\"batch_size\", 32),\n",
    "            validation_split=config.get(\"validation_split\", 0.2),\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model, config):\n",
    "        n_past = config.get(\"n_past\", 5)\n",
    "        steps = config.get(\"steps\", 1)\n",
    "        self.testing_dataset = self.dataset[self.size_train - n_past : self.size_train + self.size_test]\n",
    "        self.scaled_testing_dataset = self.scaler.transform(\n",
    "            self.testing_dataset.values.reshape(-1, 1)\n",
    "        )     \n",
    "        X_test, y_test = create_sequences(self.scaled_testing_dataset, n_past, steps)\n",
    "        # Evaluate the model on the test set\n",
    "        test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "        return test_loss\n",
    "    \n",
    "    def objective(self, config):\n",
    "        n_past, epochs, batch_size, learning_rate, neurons_l1, neurons_l2, activation_function_l1, activation_function_l2 = config\n",
    "        try:\n",
    "            model_config = {\n",
    "                \"n_past\": n_past,\n",
    "                \"steps\": CONFIG[\"PREDICTION_STEPS\"],\n",
    "                \"epochs\": epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"neurons_l1\": neurons_l1,\n",
    "                \"neurons_l2\": neurons_l2,\n",
    "                \"activation_function_l1\": activation_function_l1,\n",
    "                \"activation_function_l2\": activation_function_l2,\n",
    "            }\n",
    "\n",
    "            model = self.train_model(model_config)\n",
    "            score = self.evaluate_model(model, model_config)\n",
    "            \n",
    "            if np.isnan(score):\n",
    "                return 1e6  # Large number to indicate poor performance\n",
    "\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training: {e}\")\n",
    "            return 1e6  # Large number to indicate failure\n",
    "\n",
    "    def bayesian_optimization(self, search_space, n_calls=10):\n",
    "        # Run Bayesian optimization\n",
    "        result = gp_minimize(\n",
    "            self.objective,\n",
    "            search_space,\n",
    "            n_calls=n_calls,\n",
    "            random_state=0\n",
    "        )\n",
    "\n",
    "        # Print the result\n",
    "        print(\"Best score=%.4f\" % result.fun)\n",
    "        print(\"\"\"Best parameters:\n",
    "              - n_past=%d\n",
    "              - epochs=%d\n",
    "              - batch_size=%d\n",
    "              - learning_rate=%.6f\n",
    "              - neurons_l1=%d\n",
    "              - neurons_l2=%d\n",
    "              - activation_function_l1=%s\n",
    "              - activation_function_l2=%s\"\"\" % tuple(result.x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score=0.0061\n",
      "Best parameters:\n",
      "              - n_past=21\n",
      "              - epochs=139\n",
      "              - batch_size=30\n",
      "              - learning_rate=0.038230\n",
      "              - neurons_l1=63\n",
      "              - neurons_l2=59\n",
      "              - activation_function_l1=sigmoid\n",
      "              - activation_function_l2=sigmoid\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "selected_feature = \"cpu_usage\"\n",
    "size_train, size_test = 1000, 250\n",
    "dataset = DataManager.LoadDataset(\"../../../../../../\" + BASE_DATASET_PATH)[selected_feature]\n",
    "\n",
    "# Define the search space\n",
    "search_space = [\n",
    "    Integer(5, 100, name=\"n_past\"),\n",
    "    Integer(10, 150, name=\"epochs\"),\n",
    "    Integer(16, 64, name=\"batch_size\"),\n",
    "    Real(0.001, 0.1, name=\"learning_rate\"),\n",
    "    Integer(20, 80, name=\"neurons_l1\"),\n",
    "    Integer(20, 80, name=\"neurons_l2\"),\n",
    "    Categorical(['relu', 'tanh', 'sigmoid'], name=\"activation_function_l1\"),\n",
    "    Categorical(['relu', 'tanh', 'sigmoid'], name=\"activation_function_l2\"),\n",
    "]\n",
    "tuner = RNNHyperparameterTuner(dataset, size_train, size_test)\n",
    "tuner.bayesian_optimization(search_space=search_space, n_calls=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
